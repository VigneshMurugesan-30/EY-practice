{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13161226,"sourceType":"datasetVersion","datasetId":8268213}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install fairseq2\n!pip install pydub sentencepiece \n!pip install git+https://github.com/facebookresearch/seamless_communication.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-24T17:57:26.516601Z","iopub.execute_input":"2025-09-24T17:57:26.516817Z","iopub.status.idle":"2025-09-24T18:04:00.670515Z","shell.execute_reply.started":"2025-09-24T17:57:26.516797Z","shell.execute_reply":"2025-09-24T18:04:00.669767Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import io\nimport json\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport mmap\nimport numpy\nimport soundfile\nimport torchaudio\nimport torch\n\nfrom collections import defaultdict\nfrom IPython.display import Audio, display\nfrom pathlib import Path\nfrom pydub import AudioSegment\n\nfrom seamless_communication.inference import Translator\nfrom seamless_communication.streaming.dataloaders.s2tt import SileroVADSilenceRemover","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T18:04:00.672164Z","iopub.execute_input":"2025-09-24T18:04:00.672425Z","iopub.status.idle":"2025-09-24T18:04:07.371940Z","shell.execute_reply.started":"2025-09-24T18:04:00.672402Z","shell.execute_reply":"2025-09-24T18:04:07.371146Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget \"https://d11ywzt2xtszji.cloudfront.net/SeamlessExpressive.tar.gz?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZ2w1dG9obDh0ODN4Ym91NDViYzh3amxsIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZDExeXd6dDJ4dHN6amkuY2xvdWRmcm9udC5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NTg3Nzk0MTd9fX1dfQ__&Signature=bWDs-HXUQxMg1hMThnSJ0cQeGgNF7KBu5arLKife0k8nGkA5sL6mfFMp9fezXTnE3TdB2o326To7SrRb1mUIFeTp1c3hvVrBhM9KIhThDOx55wZNdC6NCbktV5nhQt5qoWgNURsOqlNSqgZwrXhvFUfB-tcmm53JVnvipWC5qqRKdC95csVUE-mhEKp0khjY0OHtIhdIPv2aLCgPicvsDAHCb5s%7EKQ9wwx5OEoI02S6cXmhm%7EgrPy2nJcaoMDerGYZh0u0PnfmgCWhQmBAoyybPvFyUOLU2osLpzInXjW-gJf1KO-9kQiNq2mg96b6Jvr5Qi3pZBgTBLA2nV-AJiZQ__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=823543226689486\" -O /content/SeamlessExpressive.tar.gz\n\n!tar -xzvf /content/SeamlessExpressive.tar.gz\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T18:04:07.372746Z","iopub.execute_input":"2025-09-24T18:04:07.373012Z","iopub.status.idle":"2025-09-24T18:05:03.162931Z","shell.execute_reply.started":"2025-09-24T18:04:07.372997Z","shell.execute_reply":"2025-09-24T18:05:03.161968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Input and output directories\nvad_dir = \"/kaggle/input/sample-video/EY_VAD\"\noutput_dir = \"/kaggle/working/translated\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Target language\ntgt_lang = \"spa\"\n\n# Loop through all VAD segments\nfor file in sorted(os.listdir(vad_dir)):\n    if file.endswith(\".wav\"):\n        in_file = os.path.join(vad_dir, file)\n        out_file = os.path.join(output_dir, file)  # same name as input\n\n        print(f\"üîπ Translating {file} -> {out_file}\")\n\n        # Run Seamless Expressive translation\n        !expressivity_predict {in_file} --tgt_lang {tgt_lang} \\\n            --model_name seamless_expressivity --vocoder_name vocoder_pretssel \\\n            --gated-model-dir SeamlessExpressive --output_path {out_file}\n\nprint(\"\\n‚úÖ All translated files saved in:\", output_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T18:05:03.167232Z","iopub.execute_input":"2025-09-24T18:05:03.167472Z","iopub.status.idle":"2025-09-24T18:06:21.055886Z","shell.execute_reply.started":"2025-09-24T18:05:03.167448Z","shell.execute_reply":"2025-09-24T18:06:21.055106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Audio, display\nimport os\n\n# üîπ Paths to original and translated segment folders\norig_folder = \"/kaggle/input/sample-video/EY_VAD\"  # your original segments\ntrans_folder = \"/kaggle/working/translated\"  # translated segments\n\n# üîπ Get all segment files sorted\norig_files = sorted([f for f in os.listdir(orig_folder) if f.endswith(\".wav\")])\ntrans_files = sorted([f for f in os.listdir(trans_folder) if f.endswith(\".wav\")])\n\n# üîπ Play each segment pair\nfor i, (orig_f, trans_f) in enumerate(zip(orig_files, trans_files)):\n    print(f\"\\nSegment {i}: {orig_f} (Original)\")\n    display(Audio(os.path.join(orig_folder, orig_f), autoplay=False, normalize=True, rate=16000))\n    \n    print(f\"Segment {i}: {trans_f} (Translated)\")\n    display(Audio(os.path.join(trans_folder, trans_f), autoplay=False, normalize=True, rate=16000))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T18:07:05.722210Z","iopub.execute_input":"2025-09-24T18:07:05.722961Z","iopub.status.idle":"2025-09-24T18:07:05.868134Z","shell.execute_reply.started":"2025-09-24T18:07:05.722930Z","shell.execute_reply":"2025-09-24T18:07:05.867302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import shutil\n\n# # Path to your VAD folder\n# source_dir = \"/kaggle/working/translated1\"\n# output_zip = \"/kaggle/working/EY_translated\"\n\n# # Create zip of only VAD directory\n# shutil.make_archive(output_zip.replace(\".zip\", \"\"), 'zip', source_dir)\n\n# print(f\"Zipped file saved at: {output_zip}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T18:04:06.079592Z","iopub.execute_input":"2025-09-21T18:04:06.080434Z","iopub.status.idle":"2025-09-21T18:04:06.222004Z","shell.execute_reply.started":"2025-09-21T18:04:06.080400Z","shell.execute_reply":"2025-09-21T18:04:06.221282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch torchaudio numpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T18:07:25.182398Z","iopub.execute_input":"2025-09-24T18:07:25.182938Z","iopub.status.idle":"2025-09-24T18:07:28.358346Z","shell.execute_reply.started":"2025-09-24T18:07:25.182913Z","shell.execute_reply":"2025-09-24T18:07:28.357649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install openvoice-cli","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T17:08:13.185522Z","iopub.execute_input":"2025-09-24T17:08:13.185815Z","iopub.status.idle":"2025-09-24T17:08:23.017082Z","shell.execute_reply.started":"2025-09-24T17:08:13.185789Z","shell.execute_reply":"2025-09-24T17:08:23.016262Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !openvoice_cli --help","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T18:04:35.969811Z","iopub.execute_input":"2025-09-21T18:04:35.970104Z","iopub.status.idle":"2025-09-21T18:04:36.105028Z","shell.execute_reply.started":"2025-09-21T18:04:35.970052Z","shell.execute_reply":"2025-09-21T18:04:36.104344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import sys\n# !{sys.executable} -m pip show openvoice-cli","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T18:04:36.106665Z","iopub.execute_input":"2025-09-21T18:04:36.106902Z","iopub.status.idle":"2025-09-21T18:04:38.094903Z","shell.execute_reply.started":"2025-09-21T18:04:36.106878Z","shell.execute_reply":"2025-09-21T18:04:38.093971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !python -m openvoice_cli --help\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T18:04:38.096123Z","iopub.execute_input":"2025-09-21T18:04:38.096418Z","iopub.status.idle":"2025-09-21T18:04:45.430851Z","shell.execute_reply.started":"2025-09-21T18:04:38.096392Z","shell.execute_reply":"2025-09-21T18:04:45.430121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import json\n# import math\n# import shlex\n# import subprocess\n# from collections import defaultdict\n# from pathlib import Path\n\n# import torch\n# import torchaudio\n# import numpy as np\n\n# # -------- CONFIG --------\n# original_vad_dir = \"/kaggle/working/translated\"            # original VAD segments\n# diarization_json = \"/kaggle/input/sample-video/EY_VAD/speaker_labels.json\"  # pyannote output\n# translated_dir = \"/kaggle/input/sample-video/translated_segments\"   # translated segments (read-only)\n# speaker_refs_dir = \"/kaggle/working/speaker_refs\"             # will be created\n# cloned_dir = \"/kaggle/working/cloned\"                         # final outputs\n# padded_dir = \"/kaggle/working/translated_padded\"              # temp padded files\n\n# ref_seconds = 6.0          # how many seconds of reference audio to build per speaker\n# target_sr = 22050          # OpenVoice-friendly sample rate\n# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# openvoice_cli_cmd = \"python -m openvoice_cli single\"  # CLI entry\n\n# # ------------------------\n\n# # Create folders\n# os.makedirs(speaker_refs_dir, exist_ok=True)\n# os.makedirs(cloned_dir, exist_ok=True)\n# os.makedirs(padded_dir, exist_ok=True)\n\n# # --------- Helpers ---------\n# def load_mono_np(path):\n#     wav, sr = torchaudio.load(path)\n#     if wav.shape[0] > 1:\n#         wav = wav.mean(dim=0, keepdim=True)\n#     return wav.squeeze(0).cpu().numpy(), sr\n\n# def resample_np(arr, orig_sr, new_sr):\n#     if orig_sr == new_sr:\n#         return arr\n#     t = torch.from_numpy(arr).unsqueeze(0)\n#     resampler = torchaudio.transforms.Resample(orig_sr, new_sr)\n#     t2 = resampler(t)\n#     return t2.squeeze(0).cpu().numpy()\n\n# # Load diarization JSON\n# with open(diarization_json, \"r\") as f:\n#     diar = json.load(f)\n\n# # Build speaker -> occurrences mapping\n# speaker_occ = defaultdict(list)\n# for entry in diar:\n#     segfile = entry.get(\"segment_file\")\n#     for sp in entry.get(\"speakers\", []):\n#         speaker_occ[sp[\"speaker\"]].append((segfile, float(sp[\"start\"]), float(sp[\"end\"])))\n\n# print(f\"[INFO] Found {len(speaker_occ)} speakers.\")\n\n# # -------- Build speaker references ---------\n# for speaker, occs in speaker_occ.items():\n#     parts = []\n#     accum_sec = 0.0\n#     for segfile, start, end in occs:\n#         seg_path = os.path.join(original_vad_dir, segfile)\n#         if not os.path.exists(seg_path):\n#             print(f\"[WARN] missing {seg_path} ‚Äî skipping\")\n#             continue\n#         arr, sr = load_mono_np(seg_path)\n#         s_idx = int(math.floor(start * sr))\n#         e_idx = int(math.ceil(end * sr))\n#         clip = arr[s_idx:e_idx]\n#         if clip.size == 0:\n#             continue\n#         if sr != target_sr:\n#             clip = resample_np(clip, sr, target_sr)\n#             sr = target_sr\n#         parts.append(clip)\n#         accum_sec += clip.shape[0] / float(target_sr)\n#         if accum_sec >= ref_seconds:\n#             break\n\n#     if len(parts) == 0:\n#         print(f\"[WARN] No reference for {speaker} ‚Äî skipping.\")\n#         continue\n\n#     ref_audio = np.concatenate(parts, axis=0)\n#     max_samples = int(ref_seconds * target_sr)\n#     if ref_audio.shape[0] > max_samples:\n#         ref_audio = ref_audio[:max_samples]\n#     elif ref_audio.shape[0] < max_samples:\n#         pad = np.zeros(max_samples - ref_audio.shape[0], dtype=ref_audio.dtype)\n#         ref_audio = np.concatenate([ref_audio, pad], axis=0)\n\n#     # normalize\n#     peak = np.max(np.abs(ref_audio)) + 1e-9\n#     ref_audio = (ref_audio / peak) * 0.95\n\n#     # save\n#     ref_path = os.path.join(speaker_refs_dir, f\"{speaker}.wav\")\n#     tensor = torch.from_numpy(ref_audio).unsqueeze(0)\n#     torchaudio.save(ref_path, tensor, sample_rate=target_sr)\n#     print(f\"[OK] Saved ref for {speaker} -> {ref_path}\")\n\n# print(f\"[INFO] Speaker references done at: {speaker_refs_dir}\")\n\n# # -------- Primary speaker helper ---------\n# def primary_speaker_for_segment(segfile):\n#     for entry in diar:\n#         if entry.get(\"segment_file\") == segfile:\n#             if not entry.get(\"speakers\"):\n#                 return None\n#             durations = {sp[\"speaker\"]: float(sp[\"end\"])-float(sp[\"start\"]) for sp in entry[\"speakers\"]}\n#             return max(durations, key=durations.get)\n#     return None\n\n# # --------- Cloning ---------\n# translated_files = sorted([f for f in os.listdir(translated_dir) if f.endswith(\".wav\")])\n# for fname in translated_files:\n#     in_path_orig = os.path.join(translated_dir, fname)\n#     speaker = primary_speaker_for_segment(fname)\n#     if speaker is None:\n#         print(f\"[SKIP] No diarization for {fname}\")\n#         continue\n#     ref_path = os.path.join(speaker_refs_dir, f\"{speaker}.wav\")\n#     if not os.path.exists(ref_path):\n#         print(f\"[SKIP] Missing reference for speaker {speaker}\")\n#         continue\n\n#     # Load and pad if too short\n#     audio, sr = load_mono_np(in_path_orig)\n#     min_len = int(ref_seconds * target_sr * 0.2)  # arbitrary minimum for OpenVoice\n#     if audio.shape[0] < min_len:\n#         pad = np.zeros(min_len - audio.shape[0], dtype=audio.dtype)\n#         audio = np.concatenate([audio, pad], axis=0)\n\n#     tmp_path = os.path.join(padded_dir, fname)\n#     torchaudio.save(tmp_path, torch.from_numpy(audio).unsqueeze(0), sample_rate=sr)\n\n#     out_path = os.path.join(cloned_dir, fname)\n#     cmd = f\"{openvoice_cli_cmd} -i {shlex.quote(tmp_path)} -r {shlex.quote(ref_path)} -o {shlex.quote(out_path)} -d {device}\"\n#     print(f\"[RUN] Cloning {fname} -> speaker {speaker}\")\n#     try:\n#         subprocess.run(cmd, shell=True, check=True)\n#     except subprocess.CalledProcessError as e:\n#         print(f\"[ERR] OpenVoice failed for {fname} (err {e.returncode})\")\n#         continue\n\n# print(f\"[INFO] All done. Cloned files are in {cloned_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T14:48:40.221241Z","iopub.execute_input":"2025-09-21T14:48:40.221519Z","iopub.status.idle":"2025-09-21T14:51:31.114970Z","shell.execute_reply.started":"2025-09-21T14:48:40.221500Z","shell.execute_reply":"2025-09-21T14:51:31.114156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import json\n# import math\n# import shlex\n# import subprocess\n# from collections import defaultdict\n\n# import torch\n# import torchaudio\n# import numpy as np\n\n# # -------- CONFIG --------\n# original_vad_dir = \"/kaggle/input/sample-video/vad\"           \n# diarization_json = \"/kaggle/input/sample-video/vad/speaker_labels.json\"\n# translated_dir = \"/kaggle/input/sample-video/translated_segments\"\n# speaker_refs_dir = \"/kaggle/working/speaker_reference\"\n# cloned_dir = \"/kaggle/working/cloned_audio\"\n# padded_dir = \"/kaggle/working/translated_padded\"\n\n# ref_seconds = 6.0          # seconds of reference audio per speaker\n# min_duration_sec = 6.0     # minimum duration for OpenVoice input\n# target_sr = 22050          # OpenVoice-friendly sample rate\n# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# openvoice_cli_cmd = \"python -m openvoice_cli single\"  # CLI entry\n\n# # ------------------------\n# os.makedirs(speaker_refs_dir, exist_ok=True)\n# os.makedirs(cloned_dir, exist_ok=True)\n# os.makedirs(padded_dir, exist_ok=True)\n\n# # --------- Helpers ---------\n# def load_mono_np(path):\n#     wav, sr = torchaudio.load(path)\n#     if wav.shape[0] > 1:\n#         wav = wav.mean(dim=0, keepdim=True)\n#     return wav.squeeze(0).cpu().numpy(), sr\n\n# def resample_np(arr, orig_sr, new_sr):\n#     if orig_sr == new_sr:\n#         return arr\n#     t = torch.from_numpy(arr).unsqueeze(0)\n#     resampler = torchaudio.transforms.Resample(orig_sr, new_sr)\n#     t2 = resampler(t)\n#     return t2.squeeze(0).cpu().numpy()\n\n# # -------- Load diarization ---------\n# with open(diarization_json, \"r\") as f:\n#     diar = json.load(f)\n\n# speaker_occ = defaultdict(list)\n# for entry in diar:\n#     segfile = entry.get(\"segment_file\")\n#     for sp in entry.get(\"speakers\", []):\n#         speaker_occ[sp[\"speaker\"]].append((segfile, float(sp[\"start\"]), float(sp[\"end\"])))\n\n# print(f\"[INFO] Found {len(speaker_occ)} speakers.\")\n\n# # -------- Build speaker references ---------\n# for speaker, occs in speaker_occ.items():\n#     parts = []\n#     accum_sec = 0.0\n#     for segfile, start, end in occs:\n#         seg_path = os.path.join(original_vad_dir, segfile)\n#         if not os.path.exists(seg_path):\n#             continue\n#         arr, sr = load_mono_np(seg_path)\n#         s_idx = int(math.floor(start * sr))\n#         e_idx = int(math.ceil(end * sr))\n#         clip = arr[s_idx:e_idx]\n#         if clip.size == 0:\n#             continue\n#         if sr != target_sr:\n#             clip = resample_np(clip, sr, target_sr)\n#             sr = target_sr\n#         parts.append(clip)\n#         accum_sec += clip.shape[0] / float(target_sr)\n#         if accum_sec >= ref_seconds:\n#             break\n\n#     if len(parts) == 0:\n#         print(f\"[WARN] No reference for {speaker} ‚Äî skipping.\")\n#         continue\n\n#     ref_audio = np.concatenate(parts, axis=0)\n#     max_samples = int(ref_seconds * target_sr)\n#     if ref_audio.shape[0] > max_samples:\n#         ref_audio = ref_audio[:max_samples]\n#     elif ref_audio.shape[0] < max_samples:\n#         pad = np.zeros(max_samples - ref_audio.shape[0], dtype=ref_audio.dtype)\n#         ref_audio = np.concatenate([ref_audio, pad], axis=0)\n\n#     peak = np.max(np.abs(ref_audio)) + 1e-9\n#     ref_audio = (ref_audio / peak) * 0.95\n\n#     ref_path = os.path.join(speaker_refs_dir, f\"{speaker}.wav\")\n#     tensor = torch.from_numpy(ref_audio).unsqueeze(0)\n#     torchaudio.save(ref_path, tensor, sample_rate=target_sr)\n#     print(f\"[OK] Saved ref for {speaker} -> {ref_path}\")\n\n# print(f\"[INFO] Speaker references done at: {speaker_refs_dir}\")\n\n# # -------- Primary speaker helper ---------\n# def primary_speaker_for_segment(segfile):\n#     for entry in diar:\n#         if entry.get(\"segment_file\") == segfile:\n#             if not entry.get(\"speakers\"):\n#                 return None\n#             durations = {sp[\"speaker\"]: float(sp[\"end\"])-float(sp[\"start\"]) for sp in entry[\"speakers\"]}\n#             return max(durations, key=durations.get)\n#     return None\n\n# # --------- Cloning with padding ---------\n# translated_files = sorted([f for f in os.listdir(translated_dir) if f.endswith(\".wav\")])\n\n# for fname in translated_files:\n#     in_path_orig = os.path.join(translated_dir, fname)\n#     speaker = primary_speaker_for_segment(fname)\n#     if speaker is None:\n#         print(f\"[SKIP] No diarization for {fname}\")\n#         continue\n\n#     ref_path = os.path.join(speaker_refs_dir, f\"{speaker}.wav\")\n#     if not os.path.exists(ref_path):\n#         print(f\"[SKIP] Missing reference for speaker {speaker}\")\n#         continue\n\n#     # Load translated audio\n#     audio, sr = load_mono_np(in_path_orig)\n\n#     # Resample if needed\n#     if sr != target_sr:\n#         audio = resample_np(audio, sr, target_sr)\n#         sr = target_sr\n\n#     # Pad short segments to minimum duration\n#     curr_duration = len(audio) / sr\n#     if curr_duration < min_duration_sec:\n#         pad_len = int((min_duration_sec - curr_duration) * sr)\n#         audio = np.concatenate([audio, np.zeros(pad_len, dtype=audio.dtype)])\n#         print(f\"[PAD] {fname} padded from {curr_duration:.2f}s -> {len(audio)/sr:.2f}s\")\n\n#     # Save padded audio\n#     tmp_path = os.path.join(padded_dir, fname)\n#     torchaudio.save(tmp_path, torch.from_numpy(audio).unsqueeze(0), sample_rate=sr)\n\n#     # Run OpenVoice CLI\n#     out_path = os.path.join(cloned_dir, fname)\n#     cmd = f\"{openvoice_cli_cmd} -i {shlex.quote(tmp_path)} -r {shlex.quote(ref_path)} -o {shlex.quote(out_path)} -d {device}\"\n#     print(f\"[RUN] Cloning {fname} -> speaker {speaker}\")\n\n#     try:\n#         subprocess.run(cmd, shell=True, check=True)\n#     except subprocess.CalledProcessError as e:\n#         print(f\"[ERR] OpenVoice failed for {fname} (err {e.returncode})\")\n#         continue\n\n# print(f\"[INFO] All done. Cloned files are in {cloned_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T15:24:37.317459Z","iopub.execute_input":"2025-09-21T15:24:37.317718Z","iopub.status.idle":"2025-09-21T15:27:16.164212Z","shell.execute_reply.started":"2025-09-21T15:24:37.317695Z","shell.execute_reply":"2025-09-21T15:27:16.163403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# from IPython.display import Audio, display\n\n# original_path = \"/kaggle/working/OUTPUT/output_audio.wav\"\n# translated_path = \"/kaggle/working/translated_full.wav\"\n\n# if os.path.exists(original_path):\n#     print(\"üéµ Original Audio:\")\n#     display(Audio(filename=original_path))\n# else:\n#     print(f\"‚ö†Ô∏è Original audio not found at {original_path}\")\n\n# if os.path.exists(translated_path):\n#     print(\"üéµ Translated Audio:\")\n#     display(Audio(filename=translated_path))\n# else:\n#     print(f\"‚ö†Ô∏è Translated audio not found at {translated_path}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-24T17:56:42.291Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import json\n# import soundfile as sf\n# import numpy as np\n\n# def trim_trailing_silence(audio, target_len, threshold=1e-4):\n#     \"\"\"\n#     Trim trailing silence if audio is longer than target_len.\n#     Keeps speech intact, only removes quiet tail.\n#     \"\"\"\n#     if len(audio) <= target_len:\n#         return audio\n\n#     # Find where amplitude falls below threshold near the end\n#     energy = np.abs(audio)\n#     cutoff = len(audio)\n#     while cutoff > target_len and cutoff > 0 and energy[cutoff-1] < threshold:\n#         cutoff -= 1\n\n#     # If still longer, hard cut at target_len (failsafe)\n#     if cutoff > target_len:\n#         cutoff = target_len\n\n#     return audio[:cutoff]\n\n\n# # --- Paths ---\n# timestamps_path = \"/kaggle/input/sample-video/EY_VAD/timestamps.json\"\n# segments_dir = \"/kaggle/working/translated\"\n# merged_audio_path = \"/kaggle/working/EY_merged_spa.wav\"\n\n# # Load timestamps\n# with open(timestamps_path, \"r\") as f:\n#     timestamps = json.load(f)\n\n# # Get reference sample rate\n# first_seg_path = os.path.join(segments_dir, \"speech_segment_0.wav\")\n# _, target_sr = sf.read(first_seg_path, dtype=\"float32\")\n\n# # Determine total samples\n# total_samples = int(timestamps[-1][\"end\"])\n# merged_audio = np.zeros(total_samples, dtype=np.float32)\n\n# print(f\"[INFO] Target SR: {target_sr}, Total length: {total_samples/target_sr:.2f}s\")\n\n# # Merge with silence trimming logic\n# for idx, seg in enumerate(timestamps):\n#     seg_path = os.path.join(segments_dir, f\"speech_segment_{idx}.wav\")\n#     if not os.path.exists(seg_path):\n#         print(f\"[WARN] Missing {seg_path}, skipping...\")\n#         continue\n\n#     audio, sr = sf.read(seg_path, dtype=\"float32\")\n#     if sr != target_sr:\n#         raise ValueError(f\"Sample rate mismatch in {seg_path}: {sr} vs {target_sr}\")\n\n#     start = int(seg[\"start\"])\n#     end = int(seg[\"end\"])\n#     slot_len = end - start\n\n#     if len(audio) > slot_len:\n#         # Silence-aware trimming\n#         audio = trim_trailing_silence(audio, slot_len)\n#         if len(audio) < slot_len:\n#             # pad again to exact slot length\n#             audio = np.pad(audio, (0, slot_len - len(audio)), mode=\"constant\")\n#         print(f\"[TRIM-SILENCE] {seg_path} reduced to {len(audio)} samples\")\n#     elif len(audio) < slot_len:\n#         # Pad with silence\n#         audio = np.pad(audio, (0, slot_len - len(audio)), mode=\"constant\")\n#         print(f\"[PAD] {seg_path} padded to {slot_len} samples\")\n\n#     # Place audio into merged buffer\n#     merged_audio[start:start+len(audio)] = audio\n#     print(f\"[MERGE] {seg_path} placed at {start/target_sr:.2f}s ‚Üí {end/target_sr:.2f}s\")\n\n# # Save final merged file\n# sf.write(merged_audio_path, merged_audio, target_sr)\n# print(f\"[DONE] Final merged audio saved at {merged_audio_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T18:08:29.772739Z","iopub.execute_input":"2025-09-24T18:08:29.773433Z","iopub.status.idle":"2025-09-24T18:08:29.869864Z","shell.execute_reply.started":"2025-09-24T18:08:29.773401Z","shell.execute_reply":"2025-09-24T18:08:29.869315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install ffmpeg-python","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T18:07:45.128554Z","iopub.execute_input":"2025-09-24T18:07:45.128814Z","iopub.status.idle":"2025-09-24T18:07:48.441547Z","shell.execute_reply.started":"2025-09-24T18:07:45.128794Z","shell.execute_reply":"2025-09-24T18:07:48.440791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Paths\ninput_video = \"/kaggle/input/sample-video/input_video.mp4\"  # Change to your actual video file path\noutput_dir = \"/kaggle/working/OUTPUT\"\nos.makedirs(output_dir, exist_ok=True)\n\noutput_audio = os.path.join(output_dir, \"output_audio.wav\")\noutput_video = os.path.join(output_dir, \"output_video.mp4\")\n\n# Extract audio as WAV\n!ffmpeg -i \"{input_video}\" -vn -ac 2 -ar 44100 -c:a pcm_s16le \"{output_audio}\" -y\n\n# Extract video (with original audio or without depending on your need)\n# Here, extracting video with original audio intact\n!ffmpeg -i \"{input_video}\" -c copy -an \"{output_video}\" -y\n\nprint(\"Separation completed!\")\nprint(\"Audio file:\", output_audio)\nprint(\"Video file:\", output_video)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T18:07:52.207651Z","iopub.execute_input":"2025-09-24T18:07:52.208706Z","iopub.status.idle":"2025-09-24T18:07:53.023177Z","shell.execute_reply.started":"2025-09-24T18:07:52.208669Z","shell.execute_reply":"2025-09-24T18:07:53.022476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# !pip install pyannote.audio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T10:10:47.048128Z","iopub.execute_input":"2025-09-22T10:10:47.048683Z","iopub.status.idle":"2025-09-22T10:12:06.736573Z","shell.execute_reply.started":"2025-09-22T10:10:47.048650Z","shell.execute_reply":"2025-09-22T10:12:06.735659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from pyannote.audio import Pipeline\n\n# # Load pre-trained speaker diarization pipeline\n# pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\")\n\n# # Path to your audio file\n# audio_file = \"/kaggle/working/OUTPUT/output_audio.wav\"\n\n# # Run diarization\n# diarization = pipeline(audio_file)\n\n# # Initialize stats\n# speaker_times = {}\n# speaker_turns = []\n# segments = []\n\n# # Collect segments and stats\n# for turn, _, speaker in diarization.itertracks(yield_label=True):\n#     start = turn.start\n#     end = turn.end\n#     duration = end - start\n#     speaker_times[speaker] = speaker_times.get(speaker, 0) + duration\n#     speaker_turns.append(speaker)\n#     segments.append((start, end, speaker))\n\n# # Total meeting time\n# meeting_start = min([s[0] for s in segments])\n# meeting_end = max([s[1] for s in segments])\n# total_meeting_time = meeting_end - meeting_start\n\n# # Number of speakers\n# num_speakers = len(speaker_times)\n\n# # Calculate dominance score (% speaking time)\n# dominance = {s: (t / total_meeting_time) * 100 for s, t in speaker_times.items()}\n\n# # Count speaker turns\n# from collections import Counter\n# turn_count = Counter(speaker_turns)\n\n# # Calculate total silence / active time\n# segments_sorted = sorted(segments, key=lambda x: x[0])\n# silence_time = 0\n# for i in range(1, len(segments_sorted)):\n#     prev_end = segments_sorted[i-1][1]\n#     curr_start = segments_sorted[i][0]\n#     gap = curr_start - prev_end\n#     if gap > 0:\n#         silence_time += gap\n# active_time = total_meeting_time - silence_time\n\n# # Print results\n# print(f\"\\nNumber of speakers: {num_speakers}\\n\")\n\n# print(\"Total speaking time per speaker:\")\n# for s, t in speaker_times.items():\n#     print(f\"{s}: {t:.2f}s\")\n\n# print(\"\\nDominance score (% of total meeting time):\")\n# for s, d in dominance.items():\n#     print(f\"{s}: {d:.2f}%\")\n\n# print(\"\\nSpeaker turn counts:\")\n# for s, c in turn_count.items():\n#     print(f\"{s}: {c} turns\")\n\n# print(f\"\\nTotal meeting time: {total_meeting_time:.2f}s\")\n# print(f\"Active speaking time: {active_time:.2f}s\")\n# print(f\"Silence time: {silence_time:.2f}s\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T10:12:06.738245Z","iopub.execute_input":"2025-09-22T10:12:06.738744Z","iopub.status.idle":"2025-09-22T10:12:23.857439Z","shell.execute_reply.started":"2025-09-22T10:12:06.738717Z","shell.execute_reply":"2025-09-22T10:12:23.856584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport os\nfrom pydub import AudioSegment\n\n# ====== INPUT FILES ======\nspeaker_json = \"/kaggle/input/sample-video/EY_VAD/speaker_labels.json\"\nvad_json = \"/kaggle/input/sample-video/EY_VAD/timestamps.json\"\ntranslated_dir = \"/kaggle/working/translated\"\noutput_file = \"/kaggle/working/translated_full1.wav\"\n\n# ====== LOAD JSONS ======\nwith open(speaker_json, \"r\") as f:\n    speaker_data = json.load(f)\n\nwith open(vad_json, \"r\") as f:\n    vad_data = json.load(f)\n\n# ====== CONVERT TO MS (assuming 16kHz) ======\nfor seg in vad_data:\n    seg[\"start\"] = int(seg[\"start\"] / 16)   # samples ‚Üí ms\n    seg[\"end\"]   = int(seg[\"end\"] / 16)\n\n# ====== FIND FULL LENGTH ======\noriginal_length = max([seg[\"end\"] for seg in vad_data])\nfinal_audio = AudioSegment.silent(duration=original_length)\n\n# ====== PLACE TRANSLATED SEGMENTS ======\nfor idx, seg in enumerate(vad_data):\n    seg_file = os.path.join(translated_dir, f\"speech_segment_{idx}.wav\")\n\n    if not os.path.exists(seg_file):\n        print(f\"‚ö†Ô∏è Missing file: {seg_file}, skipping...\")\n        continue\n\n    translated = AudioSegment.from_wav(seg_file)\n\n    # place at converted start_ms\n    start_ms = seg[\"start\"]\n    final_audio = final_audio.overlay(translated, position=start_ms)\n\n# ====== EXPORT ======\nfinal_audio.export(output_file, format=\"wav\")\nprint(f\"‚úÖ Final translated audio saved as {output_file}, length {len(final_audio)/1000:.2f} sec\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T18:12:29.210142Z","iopub.execute_input":"2025-09-24T18:12:29.210872Z","iopub.status.idle":"2025-09-24T18:12:29.348422Z","shell.execute_reply.started":"2025-09-24T18:12:29.210845Z","shell.execute_reply":"2025-09-24T18:12:29.347784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom IPython.display import Audio, display\n\ndef play_audios_in_path(path):\n    if not os.path.exists(path):\n        print(f\"‚ö†Ô∏è Path not found: {path}\")\n        return\n    \n    if os.path.isfile(path):\n        # If a single file is given\n        if path.lower().endswith((\".wav\", \".mp3\", \".flac\", \".ogg\")):\n            print(f\"üéµ Playing: {os.path.basename(path)}\")\n            display(Audio(filename=path))\n        else:\n            print(f\"‚ö†Ô∏è Not an audio file: {path}\")\n        return\n    \n    # If a directory is given\n    files = [f for f in os.listdir(path) if f.lower().endswith((\".wav\", \".mp3\", \".flac\", \".ogg\"))]\n    if not files:\n        print(f\"‚ö†Ô∏è No audio files found in {path}\")\n        return\n    \n    for f in files:\n        file_path = os.path.join(path, f)\n        print(f\"üéµ Playing: {f}\")\n        display(Audio(filename=file_path))\n\n# Example usage:\nplay_audios_in_path(\"/kaggle/working/OUTPUT/output_audio.wav\")\nplay_audios_in_path(\"/kaggle/working/translated_full1.wav\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T18:14:20.802667Z","iopub.execute_input":"2025-09-24T18:14:20.803376Z","iopub.status.idle":"2025-09-24T18:14:21.065935Z","shell.execute_reply.started":"2025-09-24T18:14:20.803349Z","shell.execute_reply":"2025-09-24T18:14:21.065114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}